
# AI Alignment Research Repository

Welcome to my **AI Alignment Research** repository! This project is dedicated to exploring, analyzing, and improving alignment between artificial intelligence models and human reasoning through a combination of **technical research** and **machine learning experiments**. The goal is to contribute to the field of **AI alignment**, focusing on understanding how well models make decisions that align with human expectations and how to measure that alignment systematically.

## Overview
This repository contains:
1. **Research Paper**: "Shared Interest: Measuring Human-AI Alignment to Identify Recurring Patterns in Model Behavior" by Angie Boggust et al. [Link to paper](https://dl.acm.org/doi/abs/10.1145/3491102.3501965)
2. **Jupyter Notebooks**: Hands-on technical analysis using the **Shared Interest** metrics for evaluating model alignment with human reasoning. [Link to notebook](./MeasuringSharedInterest (Grad Cam).ipynb)


## Purpose of This Repository
Iâ€™m passionate about the field of **AI alignment**, and this repository serves as a hub for my research and experiments. My aim is to dive deeper into:
- **Measuring Human-AI Alignment**: How to quantify alignment between AI model decisions and human logic using metrics like **Intersection over Union (IoU)**, **Ground Truth Coverage (GTC)**, and **Saliency Coverage (SC)**.
- **Recurring Patterns in Model Behavior**: Identifying patterns where AI models align with or deviate from human reasoning.
