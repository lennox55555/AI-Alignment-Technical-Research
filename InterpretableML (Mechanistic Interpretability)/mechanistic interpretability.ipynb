{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Explainable NLP (BERT & LIME)\n",
    "\n",
    "This Jupyter notebook explores mechanistic interpretability using sparse autoencoders, beginning with environment setup and data preparation, followed by the implementation and training of sparse autoencoders designed to analyze neural network activations. The core of the analysis focuses on feature extraction, interpretation of learned representations, and visualization of activation patterns, using techniques like activation maximization and feature attribution. The notebook concludes with quantitative evaluation metrics, case studies demonstrating practical applications, and best practices for interpreting the results in the context of model understanding.\n",
    "\n",
    "**Author:** Lennox Anderson\n",
    "\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Ly3ZATeNtOTvHYX2T8kUN2U8QFTgDFBM?usp=sharing)\n",
    "***"
   ],
   "id": "d423ab96a9a0561c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5108ace8310ac40b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
