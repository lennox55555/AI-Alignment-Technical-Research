# Adversarial AI Saftey Repository

Welcome to the **Adversarial AI Safety** folder. This section focuses on the various types of adversarial attacks that aim to compromise the integrity of machine learning (ML) models, causing misclassifications or poisoning the model's behavior. Understanding these attacks is crucial as we work toward aligning AI with human values and thinking.

## Overview

This folder contains:

- **Research Paper**: "Safety‐critical computer vision: an empirical survey of adversarial evasion attacks and defenses on computer vision systems" by Charles Meyers, Tommy Löfstedt, and Erik Elmroth. [Link to paper](https://link.springer.com/article/10.1007/s10462-023-10521-4)

- **Jupyter Notebooks**: Hands-on technical analysis of adversarial attacks and defenses to evaluate the robustness and vulnerability of AI models in safety-critical environments. Explore various attacks such as FGSM, PGD, and Adversarial Patch Attacks. [Link to notebooks](https://github.com/lennox55555/AI-Alignment-Technical-Research/blob/main/Adversarial%20AI%20Saftey/Adversarial%20AI%20Attacks.ipynb)

- **Patch Images**: Generated adversarial patches (targeted) that were applied to fool the model (RESNET34) during classification tasks.



## Purpose of This Repository

I’m deeply interested in the field of **AI safety and robustness**, particularly in understanding the vulnerabilities of AI systems in safety-critical applications. This repository serves as a space for my research and experiments, focusing on the impact of adversarial attacks on AI models and their alignment with human values. My aim is to explore:

- **Understanding Adversarial Attacks**: Investigating various attacks like **Fast Gradient Sign Method (FGSM)**, **Projected Gradient Descent (PGD)**, **Carlini & Wagner (C&W)**, **Few-Pixel Attacks**, **DeepFool**, and **Adversarial Patch Attacks** to see how they exploit model vulnerabilities.

- **Exploring Model Robustness**: How models react to adversarial perturbations and where they fail to align with human reasoning under different attack scenarios.

- **Developing Defense Mechanisms**: Experimenting with defense techniques such as **adversarial training**, **defensive distillation**, and **input transformations** to evaluate how well they mitigate the effects of adversarial attacks.

- **Evaluating AI Alignment**: Highlighting the gap between AI decisions and human expectations, particularly in adversarial settings, which demonstrates how far AI models are from fully aligning with human values and thought processes.

